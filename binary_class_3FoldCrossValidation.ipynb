{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# original directory with complete dataset\n",
    "original_dir =\"../input/total-cucumber-multi/Total_Cucumber_multi/Total_Cucumber\"\n",
    "\n",
    "# load .csv files that contains labels of images\n",
    "total_data = pd.read_csv('../input/total-cucumber-multi/cucumber_3.csv')\n",
    "\n",
    "labels = total_data[['label']] #labels\n",
    "names = total_data[['name']]  # filenames\n",
    "    \n",
    "# StratifiedKFold evenly splits images of all classes into folds    \n",
    "stratkf = StratifiedKFold(n_splits = 3, random_state = 7, shuffle = True)\n",
    "\n",
    "\n",
    "VAL_ACCURACIES = []  # for storing validation accuracies across each fold\n",
    "VAL_LOSSES = []   # for storing validation loss across each fold\n",
    "\n",
    "fold_count = 1 # initializing fold count to keep track of all folds\n",
    "\n",
    "# ImageDataGenerator is used for normalization of dataset and to perform Augmentations on the Data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = (1./255),\n",
    "    rotation_range=90,\n",
    "    width_shift_range=.2, \n",
    "    height_shift_range=.2,\n",
    "    zoom_range = 0.2,\n",
    "    brightness_range=(0.9,1.5),\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# No need to apply augmentation on validation data to mimic the unseen real world data\n",
    "validation_datagen = ImageDataGenerator(rescale = (1./255))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve model name acrossing each fold for storing models of each fold separately\n",
    "def getModelName(i):\n",
    "    return 'model_'+str(i)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries building model and for training and testing purposes\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model\n",
    "def createModel():\n",
    "    model = Sequential() #initialize sequential model. It allows to stack layers sequentially\n",
    "\n",
    "    # adding convolution layers and Pooling layers\n",
    "    model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(filters=512, kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "    # flattening the output vector into a single dimension so that it can be fed to dense layers for prediction\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # adding dense layers\n",
    "    model.add(Dense(units=4096,activation=\"relu\"))\n",
    "    model.add(Dense(units=4096,activation=\"relu\"))\n",
    "    #finally the output layer predicts the output label of image\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# loop across all three folds\n",
    "for tr_index, val_index in stratkf.split(names,labels):\n",
    "    training_data = total_data.iloc[tr_index]\n",
    "    validation_data = total_data.iloc[val_index]\n",
    "\n",
    "\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        training_data,\n",
    "        directory = original_dir,\n",
    "        target_size=(224, 224),\n",
    "        x_col = \"name\",\n",
    "        y_col = \"label\",\n",
    "        batch_size= 32,\n",
    "        class_mode='binary',\n",
    "        shuffle=True)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        validation_data,\n",
    "        directory = original_dir,\n",
    "        target_size=(224, 224),\n",
    "        x_col = \"name\",\n",
    "        y_col = \"label\",\n",
    "        batch_size= 32,\n",
    "        class_mode='binary',\n",
    "        shuffle=True)\n",
    "    \n",
    "    #create model structure\n",
    "    model = createModel()\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer= keras.optimizers.Adam(lr=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    #using callback functions offered by keras to avoid overfitting and to save the best model during training\n",
    "    # stops training when validation loss does not improve for consecutive 4 epochs\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', verbose=0, mode='min', patience = 4)\n",
    "    # saves best model with minimum loss\n",
    "    mcp_save = ModelCheckpoint('./'+getModelName(fold_count), save_best_only=True, monitor='val_loss', mode='min')\n",
    "    \n",
    "    # Training/fitting the model on training data, using validation data to avoid overfitting\n",
    "    history = model.fit_generator(train_generator,\n",
    "                use_multiprocessing=True,\n",
    "                workers=6,\n",
    "                steps_per_epoch=math.ceil(train_generator.n//train_generator.batch_size),\n",
    "                epochs = 50,\n",
    "                validation_steps=math.ceil(validation_generator.n//validation_generator.batch_size),\n",
    "                callbacks=[earlyStopping, mcp_save],\n",
    "                validation_data=validation_generator)\n",
    "    \n",
    "    # save model weights\n",
    "    model.load_weights(\"./model_\"+str(fold_count)+\".h5\")\n",
    "    \n",
    "    # evaluate and validate model on test data\n",
    "    result = model.evaluate_generator(generator=validation_generator,\n",
    "            steps=validation_generator.n//validation_generator.batch_size)\n",
    "\n",
    "    # store result of evaluation in a dictionary\n",
    "    result = dict(zip(model.metrics_names,result))\n",
    "\n",
    "    \n",
    "    VAL_ACCURACIES.append(result['accuracy']) #add accuracy to the list\n",
    "    VAL_LOSSES.append(result['loss'])   #add loss to the list\n",
    "    tf.keras.backend.clear_session()  #clear session\n",
    "    fold_count += 1 #increment fold count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average accuracy and loss across the folds\n",
    "avg_acc = sum(VAL_ACCURACIES)/len(VAL_ACCURACIES)\n",
    "avg_loss = sum(VAL_LOSSES)/len(VAL_LOSSES)\n",
    "\n",
    "# Display validation accuracy and loss across each fold\n",
    "for i in range(3):\n",
    "    print(\"Validation Accuracy for Iteration = \"+ str(i)+\" is \"+str(VAL_ACCURACIES[i]*100) +\"  and Loss = \"+str(VAL_LOSSES[i]))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Accuracy = \"+str(avg_acc*100) +\"  and Average Loss = \"+str(avg_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
